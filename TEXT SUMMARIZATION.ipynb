{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf6b143",
   "metadata": {},
   "source": [
    "# TEXT SUMMARIZATION PROJECT:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdddf23",
   "metadata": {},
   "source": [
    "Text summarization is the creation of a short, accurate, and fluent summary of a longer text document. Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online. This could help to discover relevant information and to consume relevant information faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe422445",
   "metadata": {},
   "source": [
    "# MODEL-1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968dae9",
   "metadata": {},
   "source": [
    "1.IMPORT THE LIBRARIES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a18dbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kumarapk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kumarapk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "import heapq\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843dd3ff",
   "metadata": {},
   "source": [
    "2.FUNCTION FOR PROCESSING ARTICLE TEXT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b6a2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(url):\n",
    "    scraped_data = urllib.request.urlopen(url)\n",
    "    article = scraped_data.read()\n",
    "    parsed_article = bs.BeautifulSoup(article, 'lxml')\n",
    "    paragraphs = parsed_article.find_all('p')\n",
    "    article_text = \"\"\n",
    "\n",
    "    for p in paragraphs:\n",
    "        article_text += p.text\n",
    "\n",
    "    article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "    article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ae694",
   "metadata": {},
   "source": [
    "3.FUNCTION FOR SUMMARIZATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c0979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(article_text):\n",
    "    # Removing special characters and digits\n",
    "    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text)\n",
    "    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "    sentence_list = nltk.sent_tokenize(article_text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formatted_article_text):\n",
    "        if word.lower() not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "\n",
    "    maximum_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n",
    "\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c1eb2",
   "metadata": {},
   "source": [
    "4.USER-DEFINED INPUT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdcce3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to summarize text from a URL or direct input? (Enter 'URL' or 'Text'): URL\n",
      "Enter the URL of the article you want to summarize: https://www.ibm.com/topics/text-summarization\n",
      "\n",
      "Summary:\n",
      "Gupta, “Abstractive summarization: An overview of the state of the art,” Expert Systems With Applications, 2019, https://www.sciencedirect.com/science/article/abs/pii/S0957417418307735 (link resides outside of ibm.com). Researchers also show how text summarization transformers can advance additional tasks, however.News News articles are a common dataset for testing and comparing text summarization techniques. Differences in sentence scoring determines which sentences to extract and which to retain.Abstractive summarization generates original summaries using sentences not found in the original text documents. Build AI applications in a fraction of the time with a fraction of the data.1 Juan-Manuel Torres-Moreno, Automatic Text Summarization, Wiley, 2014.2 Aggarwal, Machine Learning for Text, Springer. How do algorithms gauge semantic similarity?7Sentence scoring, per its name, scores each sentence in a text according to their importance to that text. Along these lines, neural topic models are another potential approach ordered information topically.2Developers use a number of evaluation metrics for text summarization. Of course, extractive text summarization may also utilize neural networks transformers—such as GPT, BERT, and BART—to create summaries.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    choice = input(\"Would you like to summarize text from a URL or direct input? (Enter 'URL' or 'Text'): \").strip().lower()\n",
    "\n",
    "    if choice == 'url':\n",
    "        url = input(\"Enter the URL of the article you want to summarize: \").strip()\n",
    "        article_text = get_article_text(url)\n",
    "    elif choice == 'text':\n",
    "        article_text = input(\"Enter the text you want to summarize: \").strip()\n",
    "    else:\n",
    "        print(\"Invalid choice. Please run the script again and enter either 'URL' or 'Text'.\")\n",
    "        exit()\n",
    "\n",
    "    summary = summarize_text(article_text)\n",
    "    print(\"\\nSummary:\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970dc11",
   "metadata": {},
   "source": [
    "# MODEL-2: EXTRACTIVE SUMMARIZATION:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c165f",
   "metadata": {},
   "source": [
    "Extractive summarization extracts unmodified sentences from the original text documents. A key difference between extractive algorithms is how they score sentence importance while reducing topical redundancy. Differences in sentence scoring determines which sentences to extract and which to retain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa6473",
   "metadata": {},
   "source": [
    "1.IMPORTING THE LIBRARIES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c6d643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kumarapk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kumarapk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import heapq\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9241070",
   "metadata": {},
   "source": [
    "2.NLTK SUMMARIZATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05167610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_summarize(text, n):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    word_frequencies = {}\n",
    "    for word in words:\n",
    "        if word not in word_frequencies:\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    " \n",
    "    max_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies:\n",
    "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_frequencies:\n",
    "                if sentence not in sentence_scores:\n",
    "                    sentence_scores[sentence] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += word_frequencies[word]\n",
    "\n",
    "    summary_sentences = heapq.nlargest(n, sentence_scores, key=sentence_scores.get)\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054847bd",
   "metadata": {},
   "source": [
    "3.OUTPUT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2334c78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Techniques for Abstractive Summarization\n",
      "Abstractive summarization often leverages advanced NLP techniques, such as:\n",
      "\n",
      "Sequence-to-Sequence Models (Seq2Seq): These models, often based on Recurrent Neural Networks (RNNs) or Transformers, are trained to convert a sequence of words (the input text) into another sequence (the summary). Unlike extractive summarization, which selects and compiles sentences or phrases directly from the source text, abstractive summarization creates new sentences that convey the main ideas, often using different words and structures. Key Characteristics of Abstractive Summarization\n",
      "Paraphrasing: Abstractive summarization rephrases the original text, potentially using synonyms and different grammatical structures to convey the same meaning.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Abstractive summarization is a technique in natural language processing (NLP) where the goal is to generate a concise and coherent summary of a given text by rephrasing the original content. Unlike extractive summarization, which selects and compiles sentences or phrases directly from the source text, abstractive summarization creates new sentences that convey the main ideas, often using different words and structures.\n",
    "Key Characteristics of Abstractive Summarization\n",
    "Paraphrasing: Abstractive summarization rephrases the original text, potentially using synonyms and different grammatical structures to convey the same meaning.\n",
    "Compression: The summary is typically shorter than the original text, focusing on the most important information.\n",
    "Understanding Context: This method attempts to understand the context and semantics of the text, allowing for more natural and human-like summaries.\n",
    "Complexity: Abstractive summarization is more computationally complex and challenging than extractive summarization because it involves natural language generation (NLG).\n",
    "Techniques for Abstractive Summarization\n",
    "Abstractive summarization often leverages advanced NLP techniques, such as:\n",
    "Sequence-to-Sequence Models (Seq2Seq): These models, often based on Recurrent Neural Networks (RNNs) or Transformers, are trained to convert a sequence of words (the input text) into another sequence (the summary).\n",
    "Attention Mechanisms: Used to focus on different parts of the input text when generating each word of the summary.\n",
    "Transformers: The architecture used in models like BERT, GPT-3, and T5, which can handle long-range dependencies and context more effectively.\n",
    "\"\"\"\n",
    "n = 3  \n",
    "summary = nltk_summarize(text, n)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e05f7",
   "metadata": {},
   "source": [
    "Thus, extractive summarization has been implemented sucessfully using NLTK module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125b1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
